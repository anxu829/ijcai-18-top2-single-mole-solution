{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征的提取思路\n",
    "\n",
    "### 首先提取31号-6号的特征\n",
    "### 由于6号当天的点击转化率和其他天不一样，所以需要单独处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符式行为编码方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def today_cvr_feature(org):\n",
    "    col = ['user_id', 'item_id', 'item_brand_id', 'shop_id', 'item_category_list', 'item_city_id',\n",
    "           'predict_category_property', 'context_page_id', 'query1', 'query']\n",
    "    data=org[org['day']==7]\n",
    "    train=data[data['is_trade']>-1]\n",
    "    predict=data[data['is_trade']<0]\n",
    "    predict=cvr(train,predict)\n",
    "    trains=[]\n",
    "    size=10\n",
    "    for i in range(size):\n",
    "        trains.append(split(train, i, size))\n",
    "    res=[]\n",
    "    res.append(predict)\n",
    "    for i in range(size):\n",
    "        res.append(cvr(pd.concat([trains[j] for j in range(size) if i !=j]).reset_index(drop=True),trains[i]))\n",
    "    data=pd.concat(res).reset_index(drop=True)\n",
    "    #data=data[['instance_id','today_user_cvr','today_item_cvr','today_brand_cvr','today_shop_cvr','today_cate_cvr','today_city_cvr','today_query_cvr']]\n",
    "    data=data.drop(col,axis=1)\n",
    "    data.to_csv('../data/today_cvr_feature.csv', index=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7号（最终日）前所有日期特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_days_feature(org):\n",
    "    # 取出7天前的所有数据\n",
    "    data=org[org['day']<7]\n",
    "    # 取出需要分析的列\n",
    "    col=['user_id','item_id','item_brand_id','shop_id','item_category_list','item_city_id','query1','query','context_page_id','predict_category_property']\n",
    "    \n",
    "    # 将第7天的数据提出来，将前面的统计数据慢慢的merge 到第七天上 \n",
    "    train=org[org['day']==7][['instance_id']+col]\n",
    "    \n",
    "    # 开始对于data信息的统计\n",
    "    # 统计每个user_id 的 成交次数 和  浏览总数\n",
    "    user=data.groupby('user_id',as_index=False)['is_trade'].agg({'user_buy':'sum','user_cnt':'count'})\n",
    "    # 统计前7天的转化率 ， 注意这里的 + 3 的平滑处理\n",
    "    user['user_7days_cvr']=(user['user_buy'])/(user['user_cnt']+3)\n",
    "    \n",
    "    # 取出分析的items 特征\n",
    "    items=col[1:]\n",
    "    \n",
    "    \n",
    "    train=pd.merge(train,user[['user_id','user_7days_cvr']],on='user_id',how='left')\n",
    "    \n",
    "    # 对各种 id 类特征工程 提取 转化率特征\n",
    "    for item in items:\n",
    "        tmp=data.groupby(item,as_index=False)['is_trade'].agg({item+'_buy':'sum',item+'_cnt':'count'})\n",
    "        tmp[item+'_7days_cvr'] = tmp[item+'_buy'] / tmp[item+'_cnt']\n",
    "        train = pd.merge(train, tmp[[item, item+'_7days_cvr']], on=item, how='left')\n",
    "        print(item)\n",
    "        \n",
    "    # 交互式提取特征\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i+1,len(items)):\n",
    "            egg=[items[i],items[j]]\n",
    "            tmp = data.groupby(egg, as_index=False)['is_trade'].agg({'_'.join(egg) + '_buy': 'sum', '_'.join(egg) + '_cnt': 'count'})\n",
    "            tmp['_'.join(egg) + '_7days_cvr'] = tmp['_'.join(egg) + '_buy'] / tmp['_'.join(egg) + '_cnt']\n",
    "            train = pd.merge(train, tmp[egg+['_'.join(egg) + '_7days_cvr']], on=egg, how='left')\n",
    "            print(egg)\n",
    "    train.drop(col, axis=1).to_csv('../data/7days_cvr_feature.csv',index=False)\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6号当天 单独的时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_day_feature(org):\n",
    "    data = org[org['day'] ==6]\n",
    "    col = ['user_id', 'item_id', 'item_brand_id', 'shop_id', 'item_category_list', 'item_city_id', 'query1', 'query','context_page_id','predict_category_property']\n",
    "    train = org[org['day'] == 7][['instance_id'] + col]\n",
    "    user = data.groupby('user_id', as_index=False)['is_trade'].agg({'user_buy': 'sum', 'user_cnt': 'count'})\n",
    "    user['user_6day_cvr'] = (user['user_buy']) / (user['user_cnt'] + 3)\n",
    "    train = pd.merge(train, user[['user_id', 'user_6day_cvr']], on='user_id', how='left')\n",
    "    items = col[1:]\n",
    "    for item in items:\n",
    "        tmp=data.groupby(item,as_index=False)['is_trade'].agg({item+'_buy':'sum',item+'_cnt':'count'})\n",
    "        tmp[item+'_6day_cvr'] = tmp[item+'_buy'] / tmp[item+'_cnt']\n",
    "        train = pd.merge(train, tmp[[item, item+'_6day_cvr']], on=item, how='left')\n",
    "        print(item)\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i+1,len(items)):\n",
    "            egg=[items[i],items[j]]\n",
    "            tmp = data.groupby(egg, as_index=False)['is_trade'].agg({'_'.join(egg) + '_buy': 'sum', '_'.join(egg) + '_cnt': 'count'})\n",
    "            tmp['_'.join(egg) + '_6day_cvr'] = tmp['_'.join(egg) + '_buy'] / tmp['_'.join(egg) + '_cnt']\n",
    "            train = pd.merge(train, tmp[egg+['_'.join(egg) + '_6day_cvr']], on=egg, how='left')\n",
    "            print(egg)\n",
    "    train.drop(col, axis=1).to_csv('../data/6day_cvr_feature.csv',index=False)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于cvr 计算结果统计rank 类信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计前6天的转化率对应的排名信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_7days_feature(all_days_feature(org))\n",
    "def rank_7days_feature(data):\n",
    "    data['user_cvr_brand_7days_rank']=data.groupby('item_brand_id')['user_7days_cvr'].rank(ascending=False,method='dense')\n",
    "    data['user_cvr_shop_7days_rank'] = data.groupby('shop_id')['user_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_cate_7days_rank'] = data.groupby('item_category_list')['user_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_city_7days_rank'] = data.groupby('item_city_id')['user_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_shop_7days_rank'] = data.groupby('shop_id')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_brand_7days_rank'] = data.groupby('item_brand_id')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_cate_7days_rank'] = data.groupby('item_category_list')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_city_7days_rank'] = data.groupby('item_city_id')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_brand_7days_rank'] = data.groupby('item_brand_id')['shop_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_cate_7days_rank'] = data.groupby('item_category_list')['shop_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_city_7days_rank'] = data.groupby('item_city_id')['shop_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_city_7days_rank'] = data.groupby('item_city_id')['item_brand_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_shop_7days_rank'] = data.groupby('shop_id')['item_brand_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_city_7days_rank'] = data.groupby('item_city_id')['item_category_list_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_shop_7days_rank'] = data.groupby('shop_id')['item_category_list_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query_7days_rank'] = data.groupby('query')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query1_7days_rank'] = data.groupby('query1')['item_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query_7days_rank'] = data.groupby('query')['shop_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query1_7days_rank'] = data.groupby('query1')['shop_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query_7days_rank'] = data.groupby('query')['item_brand_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query1_7days_rank'] = data.groupby('query1')['item_brand_id_7days_cvr'].rank(ascending=False, method='dense')\n",
    "    data=data[['instance_id','user_cvr_brand_7days_rank','user_cvr_shop_7days_rank','user_cvr_cate_7days_rank','user_cvr_city_7days_rank','item_cvr_shop_7days_rank','item_cvr_brand_7days_rank','item_cvr_cate_7days_rank','item_cvr_city_7days_rank','shop_cvr_brand_7days_rank','shop_cvr_cate_7days_rank','shop_cvr_city_7days_rank','brand_cvr_city_7days_rank','brand_cvr_shop_7days_rank','cate_cvr_city_7days_rank','cate_cvr_shop_7days_rank','item_cvr_query_7days_rank','item_cvr_query1_7days_rank','shop_cvr_query_7days_rank','shop_cvr_query1_7days_rank','brand_cvr_query_7days_rank','brand_cvr_query1_7days_rank'\n",
    "    ]]\n",
    "    data.to_csv('../data/rank_feature_7days.csv',index=False)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计第6天的转化率的排名信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_6day_feature(latest_day_feature(org))\n",
    "def rank_6day_feature(data):\n",
    "    data['user_cvr_brand_6day_rank']=data.groupby('item_brand_id')['user_6day_cvr'].rank(ascending=False,method='dense')\n",
    "    data['user_cvr_shop_6day_rank'] = data.groupby('shop_id')['user_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_cate_6day_rank'] = data.groupby('item_category_list')['user_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_city_6day_rank'] = data.groupby('item_city_id')['user_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_shop_6day_rank'] = data.groupby('shop_id')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_brand_6day_rank'] = data.groupby('item_brand_id')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_cate_6day_rank'] = data.groupby('item_category_list')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_city_6day_rank'] = data.groupby('item_city_id')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_brand_6day_rank'] = data.groupby('item_brand_id')['shop_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_cate_6day_rank'] = data.groupby('item_category_list')['shop_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_city_6day_rank'] = data.groupby('item_city_id')['shop_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_city_6day_rank'] = data.groupby('item_city_id')['item_brand_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_shop_6day_rank'] = data.groupby('shop_id')['item_brand_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_city_6day_rank'] = data.groupby('item_city_id')['item_category_list_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_shop_6day_rank'] = data.groupby('shop_id')['item_category_list_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query_6day_rank'] = data.groupby('query')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query1_6day_rank'] = data.groupby('query1')['item_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query_6day_rank'] = data.groupby('query')['shop_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query1_6day_rank'] = data.groupby('query1')['shop_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query_6day_rank'] = data.groupby('query')['item_brand_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query1_6day_rank'] = data.groupby('query1')['item_brand_id_6day_cvr'].rank(ascending=False, method='dense')\n",
    "    data=data[['instance_id','user_cvr_brand_6day_rank','user_cvr_shop_6day_rank','user_cvr_cate_6day_rank','user_cvr_city_6day_rank','item_cvr_shop_6day_rank','item_cvr_brand_6day_rank','item_cvr_cate_6day_rank','item_cvr_city_6day_rank','shop_cvr_brand_6day_rank','shop_cvr_cate_6day_rank','shop_cvr_city_6day_rank','brand_cvr_city_6day_rank','brand_cvr_shop_6day_rank','cate_cvr_city_6day_rank','cate_cvr_shop_6day_rank','item_cvr_query_6day_rank','item_cvr_query1_6day_rank','shop_cvr_query_6day_rank','shop_cvr_query1_6day_rank','brand_cvr_query_6day_rank','brand_cvr_query1_6day_rank'\n",
    "    ]]\n",
    "    data.to_csv('../data/rank_feature_6day.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 当天内特征的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据分成10份，从当天内其他的数据中获取当天的统计信息，然后merge到这部分子数据上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cvr(c_data, j_data):\n",
    "    col=['user_id','item_id','item_brand_id','shop_id','item_category_list','item_city_id','predict_category_property','context_page_id', 'query1', 'query']\n",
    "    \n",
    "    # 取出了后一部分数据的子列\n",
    "    j_data=j_data[['instance_id']+col]\n",
    "    \n",
    "    # 取出了第一部分数据的用户的转化率信息\n",
    "    user = c_data.groupby('user_id', as_index=False)['is_trade'].agg({'user_buy': 'sum', 'user_cnt': 'count'})\n",
    "    user['user_today_cvr'] = (user['user_buy']) / (user['user_cnt'] + 3)\n",
    "    \n",
    "    # merge到第二部分数据中\n",
    "    j_data = pd.merge(j_data, user[['user_id', 'user_today_cvr']], on='user_id', how='left')\n",
    "    \n",
    "    for item in col[1:]:\n",
    "        tmp=c_data.groupby(item, as_index=False)['is_trade'].agg({item+'_today_cvr': 'mean'})\n",
    "        j_data = pd.merge(j_data, tmp, on=item, how='left')\n",
    "    for i in range(len(col)):\n",
    "        for j in range(i+1,len(col)):\n",
    "            tmp=c_data.groupby([col[i],col[j]], as_index=False)['is_trade'].agg({'today_'+col[i]+col[j]+'_cvr': 'mean'})\n",
    "            j_data = pd.merge(j_data, tmp, on=[col[i],col[j]], how='left')\n",
    "            print([col[i],col[j]])\n",
    "    return j_data\n",
    "\n",
    "\n",
    "# [['instance_id','today_user_cvr','today_item_cvr','today_brand_cvr','today_shop_cvr','today_cate_cvr','today_city_cvr']]\n",
    "\n",
    "def split(data, index, size):\n",
    "    import math\n",
    "    size = math.ceil(len(data) / size)\n",
    "    start = size * index\n",
    "    end = (index + 1) * size if (index + 1) * size < len(data) else len(data)\n",
    "    return data[start:end]\n",
    "\n",
    "\n",
    "\n",
    "def today_cvr_feature(org):\n",
    "    col = ['user_id', 'item_id', 'item_brand_id', 'shop_id', 'item_category_list', 'item_city_id',\n",
    "           'predict_category_property', 'context_page_id', 'query1', 'query']\n",
    "    data=org[org['day']==7]\n",
    "    # 获取第七天上午有交易信息的部分\n",
    "    \n",
    "    train=data[data['is_trade']>-1]\n",
    "    \n",
    "    # 这部分是没有交易信息的部分\n",
    "    predict=data[data['is_trade']<0]\n",
    "    \n",
    "    predict=cvr(train,predict)\n",
    "    \n",
    "    \n",
    "    trains=[]\n",
    "    size=10\n",
    "    for i in range(size):\n",
    "        # 又把train拆成了很多份\n",
    "        trains.append(split(train, i, size))\n",
    "    \n",
    "    # 把predict 放到了res中\n",
    "    res=[]\n",
    "    res.append(predict)\n",
    "    for i in range(size):\n",
    "        # 对每一份train 的子数据， 用其他部分数据 计算出来的转化率 作为这部分子数据的表征\n",
    "        res.append(cvr(pd.concat([trains[j] for j in range(size) if i !=j]).reset_index(drop=True),trains[i]))\n",
    "    data=pd.concat(res).reset_index(drop=True)\n",
    "    #data=data[['instance_id','today_user_cvr','today_item_cvr','today_brand_cvr','today_shop_cvr','today_cate_cvr','today_city_cvr','today_query_cvr']]\n",
    "    data=data.drop(col,axis=1)\n",
    "    data.to_csv('../data/today_cvr_feature.csv', index=False)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def rank_today_feature(data):\n",
    "    data=data.reset_index(drop=True)\n",
    "    data['user_cvr_brand_today_rank']=data.groupby('item_brand_id')['user_today_cvr'].rank(ascending=False,method='dense')\n",
    "    data['user_cvr_shop_today_rank'] = data.groupby('shop_id')['user_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_cate_today_rank'] = data.groupby('item_category_list')['user_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['user_cvr_city_today_rank'] = data.groupby('item_city_id')['user_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_shop_today_rank'] = data.groupby('shop_id')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_brand_today_rank'] = data.groupby('item_brand_id')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_cate_today_rank'] = data.groupby('item_category_list')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_city_today_rank'] = data.groupby('item_city_id')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_brand_today_rank'] = data.groupby('item_brand_id')['shop_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_cate_today_rank'] = data.groupby('item_category_list')['shop_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_city_today_rank'] = data.groupby('item_city_id')['shop_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_city_today_rank'] = data.groupby('item_city_id')['item_brand_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_shop_today_rank'] = data.groupby('shop_id')['item_brand_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_city_today_rank'] = data.groupby('item_city_id')['item_category_list_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['cate_cvr_shop_today_rank'] = data.groupby('shop_id')['item_category_list_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query_today_rank'] = data.groupby('query')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['item_cvr_query1_today_rank'] = data.groupby('query1')['item_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query_today_rank'] = data.groupby('query')['shop_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['shop_cvr_query1_today_rank'] = data.groupby('query1')['shop_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query_today_rank'] = data.groupby('query')['item_brand_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data['brand_cvr_query1_today_rank'] = data.groupby('query1')['item_brand_id_today_cvr'].rank(ascending=False, method='dense')\n",
    "    data=data[['instance_id','user_cvr_brand_today_rank','user_cvr_shop_today_rank','user_cvr_cate_today_rank','user_cvr_city_today_rank','item_cvr_shop_today_rank','item_cvr_brand_today_rank','item_cvr_cate_today_rank','item_cvr_city_today_rank','shop_cvr_brand_today_rank','shop_cvr_cate_today_rank','shop_cvr_city_today_rank','brand_cvr_city_today_rank','brand_cvr_shop_today_rank','cate_cvr_city_today_rank','cate_cvr_shop_today_rank','item_cvr_query_today_rank','item_cvr_query1_today_rank','shop_cvr_query_today_rank','shop_cvr_query1_today_rank','brand_cvr_query_today_rank','brand_cvr_query1_today_rank'\n",
    "    ]]\n",
    "    data.to_csv('../data/rank_feature_today.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
